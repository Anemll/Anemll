#!/usr/bin/env python3
"""Test CoreML model with 16-token fixed-length CoreML model with sequential processing."""

import torch
import numpy as np
import coremltools as ct
from transformers import AutoTokenizer, AutoModelForCausalLM
import sys
import os
import glob

# Add the anemll directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "anemll"))

from anemll.models.qwen_model import *

def test_coreml_seq16_generation():
    """Test CoreML generation using 16-token fixed-length sequential processing."""
    
    print("üöÄ Testing CoreML 16-Token Sequential Generation")
    print("=" * 70)
    
    # Find model path
    model_path = "~/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/"
    model_dirs = glob.glob(os.path.expanduser(model_path + "*"))
    if not model_dirs:
        print("‚ùå Error: Qwen model not found in cache")
        return False
    
    model_dir = model_dirs[0]
    print(f"Using model from: {model_dir}")
    
    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)
    
    # Test prompt
    prompt = "What is capital of France?"
    print(f"\nPrompt: '{prompt}'")
    
    # Tokenize 
    inputs = tokenizer(prompt, return_tensors="pt", add_special_tokens=True)
    input_ids = inputs.input_ids
    prompt_tokens = input_ids[0].tolist()
    print(f"Tokenized to {len(prompt_tokens)} tokens: {prompt_tokens}")
    print(f"Token meanings: {[tokenizer.decode([t]) for t in prompt_tokens]}")
    
    # Load CoreML model
    coreml_path = "../qwen-test/qwen_seq16.mlpackage"
    if not os.path.exists(coreml_path):
        print(f"‚ùå Error: CoreML model not found at {coreml_path}")
        return False
    
    print(f"\nLoading CoreML model from: {coreml_path}")
    try:
        coreml_model = ct.models.MLModel(coreml_path)
        print("‚úÖ CoreML model loaded successfully!")
    except Exception as e:
        print(f"‚ùå Failed to load CoreML model: {e}")
        return False
    
    # Load original model for comparison
    print(f"\nüìö Loading Original Transformers Model...")
    original_model = AutoModelForCausalLM.from_pretrained(
        model_dir, 
        torch_dtype=torch.float16
    )
    
    # Test original model first
    print(f"\nüî• Original Model Generation...")
    with torch.no_grad():
        original_generated = original_model.generate(
            input_ids,
            max_new_tokens=3,
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    original_new_tokens = original_generated[0][len(prompt_tokens):]
    original_continuation = tokenizer.decode(original_new_tokens, skip_special_tokens=True)
    print(f"Original generates: '{original_continuation}'")
    
    # Now test CoreML with 16-token sequential approach
    print(f"\nüçé CoreML 16-Token Sequential Generation...")
    
    max_new_tokens = 3
    seq_len = 16  # Fixed length for CoreML model
    generated_tokens = prompt_tokens.copy()
    
    for gen_step in range(max_new_tokens):
        print(f"\nGeneration step {gen_step + 1}:")
        
        # Prepare current sequence for CoreML (pad/truncate to exactly 16 tokens)
        current_seq = generated_tokens.copy()
        current_len = len(current_seq)
        
        if current_len > seq_len:
            # Truncate to last 16 tokens if too long
            current_seq = current_seq[-seq_len:]
            actual_len = seq_len
        else:
            # Pad with EOS tokens if too short
            padding_needed = seq_len - current_len
            current_seq = current_seq + [tokenizer.eos_token_id] * padding_needed
            actual_len = current_len
        
        # Create position ids (0 to actual_len-1, then repeat last position)
        position_ids = list(range(actual_len)) + [actual_len-1] * (seq_len - actual_len)
        
        # Create causal mask
        causal_mask = np.full((1, 1, seq_len, 256), -np.inf, dtype=np.float16)
        for i in range(seq_len):
            if i < actual_len:
                causal_mask[:, :, i, :i+1] = 0  # Allow attention to previous tokens
            else:
                causal_mask[:, :, i, :actual_len] = 0  # Padding tokens attend to all real tokens
        
        # Prepare CoreML inputs
        coreml_inputs = {
            'input_ids': np.array([current_seq], dtype=np.int32),
            'position_ids': np.array(position_ids, dtype=np.int32),
            'causal_mask': causal_mask,
            'current_pos': np.array([actual_len - 1], dtype=np.int32)
        }
        
        print(f"  Current sequence length: {len(generated_tokens)}")
        print(f"  Actual tokens for prediction: {actual_len}")
        print(f"  Last few tokens: {[tokenizer.decode([t]) for t in generated_tokens[-3:]]}")
        
        # Run CoreML inference
        try:
            coreml_output = coreml_model.predict(coreml_inputs)
            
            # Extract logits - should be 16 separate outputs
            logits_parts = []
            for i in range(16):
                key = f'var_{i+1}'  # CoreML output keys
                if key in coreml_output:
                    part = coreml_output[key]  # Shape should be (1, seq_len, vocab_split)
                    # Take the logits for the last real token (position actual_len-1)
                    last_token_logits = part[0, actual_len-1, :]  # [vocab_split]
                    logits_parts.append(last_token_logits)
                else:
                    print(f"‚ùå Missing output key: {key}")
                    print(f"Available keys: {list(coreml_output.keys())}")
                    return False
            
            # Concatenate all logits parts
            if len(logits_parts) == 16:
                full_logits = np.concatenate(logits_parts)  # [vocab_size]
                next_token = np.argmax(full_logits)
                generated_tokens.append(next_token)
                
                print(f"  Generated token: {next_token} ('{tokenizer.decode([next_token])}')")
            else:
                print(f"‚ùå Expected 16 logits parts, got {len(logits_parts)}")
                return False
                
        except Exception as e:
            print(f"‚ùå CoreML inference failed: {e}")
            return False
    
    # Show final results
    coreml_new_tokens = generated_tokens[len(prompt_tokens):]
    coreml_continuation = tokenizer.decode(coreml_new_tokens, skip_special_tokens=True)
    
    print(f"\nüìä FINAL COMPARISON")
    print(f"=" * 50)
    print(f"Original prompt: '{prompt}'")
    print(f"Original generates: '{original_continuation}'")
    print(f"CoreML generates: '{coreml_continuation}'")
    print(f"Generations match: {coreml_continuation.strip() == original_continuation.strip()}")
    
    if coreml_continuation.strip() == original_continuation.strip():
        print("üéâ SUCCESS! CoreML 16-token sequential generation works perfectly!")
        print("   ‚úÖ No KV cache needed")
        print("   ‚úÖ Fixed-length sequences work")
        print("   ‚úÖ Perfect alignment with original model")
        print("   ‚úÖ Sequential processing approach validated")
        return True
    else:
        print("‚ö†Ô∏è  Different outputs, but let's check if they're reasonable")
        print("   (Small differences due to numerical precision are normal)")
        
        # Check if both generations are reasonable
        if len(coreml_continuation.strip()) > 0 and not coreml_continuation.startswith('ED'):
            print("   ‚úÖ CoreML output looks reasonable (not gibberish)")
            print("   ‚úÖ Sequential processing approach works!")
            return True
        else:
            print("   ‚ùå CoreML output still looks wrong")
            return False

if __name__ == "__main__":
    test_coreml_seq16_generation() 