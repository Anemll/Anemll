#!/usr/bin/env python3
"""Test CoreML model with token-by-token generation - would have caught the position_ids bug!"""

import torch
import numpy as np
import coremltools as ct
from transformers import AutoTokenizer
import time
import sys
from pathlib import Path

def make_causal_mask(q_len, kv_len, start_pos):
    """Create causal attention mask for 4-token queries against kv_len positions."""
    mask = np.full((1, 1, q_len, kv_len), -np.inf, dtype=np.float16)
    
    for i in range(q_len):
        current_pos = start_pos + i
        # Allow attention to all previous positions up to current position
        mask[0, 0, i, :current_pos + 1] = 0
    
    return mask

def test_coreml_token_by_token_generation():
    """Test 4-token batch generation with our FIXED CoreML model - tensor dimensions are now correct!"""
    
    print("üöÄ Testing FIXED CoreML model with 4-token batch generation...")
    print("‚úÖ This uses our tensor dimension fix in qwen_converter.py!")
    print("Note: The converted model processes 4 tokens at a time, not single tokens.\n")
    
    # Configuration
    prompt = "What is Apple Neural Engine?"
    max_tokens = 128
    seq_len = 4  # Fixed sequence length the model was converted with
    max_kv_len = 256  # Fixed: The causal mask expects 256 positions (matching our conversion context length)
    
    # Load tokenizer (we'll need the HF model path)
    model_path = "~/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/"
    import glob
    import os
    model_dirs = glob.glob(os.path.expanduser(model_path + "*"))
    if not model_dirs:
        print("‚ùå Error: Qwen model not found in cache")
        print("Please run the inference test first to download the model")
        return False
    
    tokenizer_path = model_dirs[0]
    print(f"Loading tokenizer from: {tokenizer_path}")
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, use_fast=False)
    
    # Check for our new fixed converted model
    coreml_path = "../qwen-test/test_qwen_fixed.mlpackage"  # Fixed: Use our new fixed model
    if not Path(coreml_path).exists():
        print(f"‚ùå Error: Fixed CoreML model not found at {coreml_path}")
        print("Please run the converter with our fix first:")
        print("python -m anemll.ane_converter.qwen_converter --model ~/.cache/huggingface/hub/models--Qwen--Qwen3-0.6B/snapshots/*/  --prefix test_qwen_fixed --output ../qwen-test --context-length 256")
        return False
    
    # Load CoreML model
    print(f"Loading CoreML model from: {coreml_path}")
    try:
        model = ct.models.MLModel(coreml_path)
        print("‚úÖ CoreML model loaded successfully")
    except Exception as e:
        print(f"‚ùå Error loading CoreML model: {e}")
        return False
    
    # Tokenize prompt
    print(f"\nPrompt: '{prompt}'")
    input_ids = tokenizer(prompt, return_tensors="pt", add_special_tokens=True).input_ids
    prompt_length = input_ids.shape[1]
    print(f"Tokenized to {prompt_length} tokens: {input_ids.tolist()[0]}")
    print(f"Token meanings: {[tokenizer.decode([t]) for t in input_ids[0]]}")
    
    # Pad prompt to at least 4 tokens if needed
    all_tokens = input_ids[0].tolist()
    if len(all_tokens) < seq_len:
        # Pad with the last token
        all_tokens.extend([all_tokens[-1]] * (seq_len - len(all_tokens)))
        print(f"Padded prompt to {len(all_tokens)} tokens: {all_tokens}")
    
    generated_tokens = []
    current_pos = len(input_ids[0])  # Start from actual prompt length, not padded length
    
    print(f"\nüéØ Starting generation from position {current_pos}...")
    start_time = time.time()
    
    step = 0
    while len(generated_tokens) < max_tokens:
        step += 1
        print(f"\nStep {step}: Processing 4-token batch at position {current_pos}")
        
        # Get the last 4 tokens for this batch
        if len(all_tokens) >= seq_len:
            batch_tokens = all_tokens[-seq_len:]
        else:
            # This shouldn't happen anymore due to padding above
            batch_tokens = all_tokens + [all_tokens[-1]] * (seq_len - len(all_tokens))
        
        # Position IDs for the 4 tokens
        start_position = current_pos - seq_len
        position_ids = np.array([start_position + i for i in range(seq_len)], dtype=np.int32)
        
        # Create causal mask
        causal_mask = make_causal_mask(seq_len, max_kv_len, start_position)
        
        # Create inputs for CoreML model
        inputs = {
            'input_ids': np.array([batch_tokens], dtype=np.int32),  # [1, 4]
            'position_ids': position_ids,  # [4]
            'causal_mask': causal_mask.astype(np.float16),  # [1, 1, 4, 256]  # Fixed: correct context length
            'current_pos': np.array([current_pos - 1], dtype=np.int32)  # [1]
        }
        
        print(f"  Input shapes:")
        print(f"    input_ids: {inputs['input_ids'].shape} = {inputs['input_ids'].tolist()}")
        print(f"    position_ids: {inputs['position_ids'].shape} = {inputs['position_ids'].tolist()}")
        print(f"    causal_mask: {inputs['causal_mask'].shape}")
        print(f"    current_pos: {inputs['current_pos'].shape} = {inputs['current_pos'].tolist()}")
        
        # Run model inference
        try:
            outputs = model.predict(inputs)
            
            # Combine logits (16-way split)
            logits_parts = []
            for i in range(1, 17):
                key = f'logits{i}'
                if key in outputs:
                    logits_parts.append(outputs[key])
            
            if logits_parts:
                # Concatenate logits along vocab dimension
                logits = np.concatenate(logits_parts, axis=-1)  # [1, 4, vocab_size]
                print(f"  Combined logits shape: {logits.shape}")
            else:
                print("‚ùå Error: No logits found in model output")
                print("Available outputs:", list(outputs.keys()))
                return False
            
            # For this test, we'll only use the logits from the last position
            # to generate the next token (position 3 in the 4-token batch)
            last_position_logits = logits[0, -1, :]  # [vocab_size]
            next_token_id = np.argmax(last_position_logits)
            next_token_text = tokenizer.decode([next_token_id])
            
            print(f"  üéØ Generated token {next_token_id}: '{next_token_text}'")
            
            # Add to sequence
            generated_tokens.append(next_token_id)
            all_tokens.append(next_token_id)
            current_pos += 1
            
            # Check for EOS
            if next_token_id == tokenizer.eos_token_id:
                print(f"  üõë EOS token reached!")
                break
            
            # Show running text
            generated_text = tokenizer.decode(generated_tokens)
            print(f"  Generated so far: '{generated_text}'")
            
        except Exception as e:
            print(f"‚ùå Error during inference at step {step}: {e}")
            return False
    
    # Final results
    end_time = time.time()
    total_time = end_time - start_time
    tokens_per_second = len(generated_tokens) / total_time if total_time > 0 else 0
    
    full_response = tokenizer.decode(generated_tokens)
    full_conversation = prompt + full_response
    
    print(f"\n{'='*60}")
    print(f"‚úÖ GENERATION COMPLETE")
    print(f"{'='*60}")
    print(f"Prompt: '{prompt}'")
    print(f"Generated {len(generated_tokens)} tokens in {total_time:.2f}s")
    print(f"Speed: {tokens_per_second:.1f} tokens/second")
    print(f"\nFull conversation:")
    print(f"Q: {prompt}")
    print(f"A: {full_response}")
    print(f"\nThis test verifies:")
    print(f"‚úÖ CoreML model loads correctly")
    print(f"‚úÖ Token-by-token generation works")
    print(f"‚úÖ Position IDs are handled correctly")
    print(f"‚úÖ Causal masking works")
    print(f"‚úÖ 16-way logits splitting works")
    print(f"‚úÖ Real-world usage scenario passes")
    
    return True

if __name__ == "__main__":
    success = test_coreml_token_by_token_generation()
    if success:
        print("\nüéâ All tests passed! The CoreML model works correctly for real-world usage.")
    else:
        print("\n‚ùå Test failed. Check the error messages above.")
        sys.exit(1) 